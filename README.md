# ðŸ§  Offensive Language Detection using NLP and Machine Learning

This project focuses on **detecting offensive language** in text data using **Natural Language Processing (NLP)** and **Machine Learning** techniques.  
It classifies input text as **offensive** or **non-offensive**, helping moderate online platforms and create a safer communication environment.

---

## ðŸš€ Features

- ðŸ§© Detects if a given text is **offensive** or **non-offensive**
- ðŸ§  Uses **NLP preprocessing** and **ML classification models**
- ðŸ“Š Provides **accuracy, precision, recall, F1-score, confusion matrix**
- ðŸ“ˆ Includes **visualizations** for model performance and predictions
- ðŸ’» Fully implemented in a **Jupyter / Google Colab notebook**

---

## ðŸ§° Tech Stack

| Category | Tools / Libraries |
|-----------|-------------------|
| Language | Python |
| Environment | Google Colab / Jupyter Notebook |
| NLP | NLTK, spaCy, TfidfVectorizer |
| ML Models | Logistic Regression, Random Forest, SVM |
| Data Handling | pandas, numpy |
| Visualization | matplotlib, seaborn, wordcloud |

---

## ðŸ“Š Model Performance

**Model Accuracy on Test Dataset:**  
`Accuracy: 93.50%`

You can visualize the model's performance using the plot below.

### ðŸ”¹ Model Accuracy Plot
![Model Accuracy](images/model_accuracy.png)

## ðŸ“ˆ Actual vs Predicted Values

The plot below shows how close the modelâ€™s predictions are to the actual values.

![Actual vs Predicted Values](images/actual_vs_predicted.png)

---

## ðŸ’¬ Model Output 

This section demonstrates how the trained model classifies text inputs as **Offensive** or **Non-Offensive**.




